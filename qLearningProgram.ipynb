{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKCt3dwnownnMblF3b5cBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryanjha256/randomjupyternotebooks/blob/main/qLearningProgram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCUd1wIj7p7z",
        "outputId": "e39e51af-8520-476a-ce75-01f18d8d7021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-table: \n",
            "[[  1.48134446  -0.72183154   1.63802417   3.122     ]\n",
            " [  2.81004737 -94.76652367   1.64966105   4.58      ]\n",
            " [  3.9413895    5.49975842   3.00445177   6.2       ]\n",
            " [  5.38708801   8.           3.82282618   6.04621621]\n",
            " [ -1.00225932   1.48250957  -1.13213616 -27.1       ]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ -0.1          1.91529155 -10.           7.95327876]\n",
            " [  5.82827798  10.           4.9469574    7.44877101]\n",
            " [ -0.67640616  -0.67078761  -0.67934652   4.28607438]\n",
            " [-10.          -0.2962      -0.231319     7.22380292]\n",
            " [ -0.1          1.38442475  -0.21439      9.86697205]\n",
            " [  0.           0.           0.           0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "env = np.array([['S', '.', '.', ','],\n",
        "                ['.', 'C', '.', '.'],\n",
        "                ['.', '.', '.', 'G']])\n",
        "\n",
        "# Prameters\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "exploration_prob = 0.1\n",
        "num_episodes = 1000\n",
        "\n",
        "# Initialize the Q-table with zeroes\n",
        "num_states = np.prod(env.shape)\n",
        "num_actions = 4 # up, down, left, right\n",
        "q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Helper function to convert state coordinates to a single index\n",
        "def state_to_index(state, env_shape):\n",
        "    return state[0] * env_shape[1] + state[1]\n",
        "\n",
        "# Helper function to choose an action based on Q-table\n",
        "def choose_action(state):\n",
        "    if np.random.uniform(0 ,1) < exploration_prob:\n",
        "        return np.random.choice(num_actions) # Random action\n",
        "    else:\n",
        "        return np.argmax(q_table[state_to_index(state, env.shape), :])\n",
        "\n",
        "# Helper function to get the next state based on the current state and action\n",
        "def get_next_state(state, action):\n",
        "    if action == 0 and state[0] > 0:\n",
        "        return (state[0] - 1,  state[1]) # Up\n",
        "    elif action == 1 and state[0] < env.shape[0] - 1:\n",
        "        return (state[0] + 1,  state[1]) # Down\n",
        "    elif action == 2 and state[1] > 0:\n",
        "        return (state[0],  state[1] -1) # Left\n",
        "    elif action == 3 and state[1] < env.shape[1] - 1:\n",
        "        return (state[0],  state[1] + 1) # Right\n",
        "    else:\n",
        "        return state\n",
        "\n",
        "\n",
        "# Q-Learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = (0, 0) # Start state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(state)\n",
        "        next_state = get_next_state(state, action)\n",
        "\n",
        "        # Calculate reward based on the next state\n",
        "        if env[next_state] == 'C':\n",
        "            reward = -100 # Cliff\n",
        "            done = True\n",
        "\n",
        "        elif env[next_state] == 'G':\n",
        "            reward = 10 # Goal\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -1 # Empty cell\n",
        "\n",
        "        # Update the Q-value for the current state-action pair\n",
        "        q_table[state_to_index(state, env.shape),  action] = (1 - learning_rate) * q_table[state_to_index(state, env.shape), action] + learning_rate * (reward + discount_factor * np.max(q_table[state_to_index(next_state, env.shape), :]))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Check if the episode is done\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "# Print the Q-table\n",
        "print(\"Learned Q-table: \")\n",
        "print(q_table)"
      ]
    }
  ]
}